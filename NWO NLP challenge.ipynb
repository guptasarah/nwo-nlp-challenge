{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: Semantic Search Algorithm  \n",
    "Design and implement a semantic search algorithm that is able to score and rank a  set of keywords (trends) by how strongly associated they are to a given query term.  The algorithmic approach could borrow techniques from association rule mining to  analyze the co-occurrence of terms within a corpora of tweets and reddit posts, and should take into consideration the uniqueness of the trend and the recency of the  association. For example, the algorithm should be able to determine that the query  ‘iPhone’ is more strongly associated to trends like ‘MagSafe’, ‘5G’, and ‘pacific blue'  then it is to “Biden” or “perfume”.  \n",
    "\n",
    "## Details:  \n",
    "- The expected input to the method should be a query term, and the output should  be an ordered set of trends. The method should be implemented in Python (v3.7).  \n",
    "- You can explore the dataset via the GCP BigQuery WebIDE and you can connect to  the database from python using the provided JSON key.  \n",
    "- The sample twitter and reddit datasets can found in the tables `nwo-sample.graph.tweets` and `nwo-sample.graph.reddit` respectively   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json access key\n",
    "credentials = service_account.Credentials.from_service_account_file('nwo-sample-5f8915fdc5ec.json')\n",
    "\n",
    "project_id = 'nwo-sample'\n",
    "client = bigquery.Client(credentials= credentials,project=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the table\n",
    "tweet_query = client.query(\"\"\"\n",
    "   SELECT `tweet` FROM nwo-sample.graph.tweets \n",
    "   LIMIT 10000\"\"\")\n",
    "tweet_results = tweet_query.result() \n",
    "\n",
    "reddit_query = client.query(\"\"\"\n",
    "   SELECT `body` FROM nwo-sample.graph.reddit \n",
    "   LIMIT 10000\"\"\")\n",
    "reddit_results = reddit_query.result() \n",
    "\n",
    "type_results = [tweet_results, reddit_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = []\n",
    "all_text = []\n",
    "\n",
    "wn = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "# go through all results, keep track of text\n",
    "# normalize text (tokenize, remove stopwords, small words, and numbers/symbols)\n",
    "for tresult in type_results:\n",
    "\n",
    "    for r in tresult:\n",
    "        tokens = []\n",
    "\n",
    "        tokenized = tokenizer.tokenize(r[0])\n",
    "        words = [word.lower() for word in tokenized if word.isalpha() and len(word)>2 and word not in stopwords.words('english')]\n",
    "        tags = {'N': wordnet.NOUN, 'J': wordnet.ADJ, 'V': wordnet.VERB, 'R': wordnet.ADV}\n",
    "       \n",
    "        pos_tags = nltk.pos_tag(words)\n",
    "        for word in pos_tags:\n",
    "            if word[1][0] in tags:\n",
    "                new_pos = tags[word[1][0]]\n",
    "                lemma = wn.lemmatize(word[0], new_pos)\n",
    "                tokens.append(lemma)\n",
    "                terms.append(lemma)\n",
    "        all_text.append(list(set(tokens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# association rule mining\n",
    "# first, convert to one-hot encoding\n",
    "encoding = []\n",
    "term_set = set(terms)\n",
    "for curr_text in all_text:\n",
    "    labels = {}\n",
    "    zeros = list(term_set - set(curr_text))\n",
    "    ones = list(term_set.intersection(set(curr_text)))\n",
    "    for missing_word in zeros:\n",
    "        labels[missing_word] = 0\n",
    "    for present_word in ones:\n",
    "        labels[present_word] = 1\n",
    "    encoding.append(labels)\n",
    "encoded_df = pd.DataFrame(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 11 combinations | Sampling itemset size 11032\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00525</td>\n",
       "      <td>(local)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00720</td>\n",
       "      <td>(american)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01240</td>\n",
       "      <td>(number)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00535</td>\n",
       "      <td>(imagine)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00555</td>\n",
       "      <td>(compose)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   support    itemsets\n",
       "0  0.00525     (local)\n",
       "1  0.00720  (american)\n",
       "2  0.01240    (number)\n",
       "3  0.00535   (imagine)\n",
       "4  0.00555   (compose)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_items = apriori(encoded_df, min_support=0.005, use_colnames=True, verbose=1)\n",
    "freq_items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keywords = {}\n",
    "for i, row in freq_items.iterrows():\n",
    "    items = list(row['itemsets'])\n",
    "    if len(items)>1:\n",
    "        if items[0] not in keywords:\n",
    "            keywords[items[0]] = {}\n",
    "        if items[1] not in keywords:\n",
    "            keywords[items[1]] = {}\n",
    "        keywords[items[0]][items[1]] = row['support']\n",
    "        keywords[items[1]][items[0]] = row['support']\n",
    "\n",
    "for word in keywords:\n",
    "    keywords[word] = sorted(keywords[word].items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(perform)</td>\n",
       "      <td>(compose)</td>\n",
       "      <td>0.00650</td>\n",
       "      <td>0.00555</td>\n",
       "      <td>0.00510</td>\n",
       "      <td>0.784615</td>\n",
       "      <td>141.372141</td>\n",
       "      <td>0.005064</td>\n",
       "      <td>4.617089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(compose)</td>\n",
       "      <td>(perform)</td>\n",
       "      <td>0.00555</td>\n",
       "      <td>0.00650</td>\n",
       "      <td>0.00510</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>141.372141</td>\n",
       "      <td>0.005064</td>\n",
       "      <td>12.253167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(message)</td>\n",
       "      <td>(compose)</td>\n",
       "      <td>0.01115</td>\n",
       "      <td>0.00555</td>\n",
       "      <td>0.00545</td>\n",
       "      <td>0.488789</td>\n",
       "      <td>88.070133</td>\n",
       "      <td>0.005388</td>\n",
       "      <td>1.945284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(compose)</td>\n",
       "      <td>(message)</td>\n",
       "      <td>0.00555</td>\n",
       "      <td>0.01115</td>\n",
       "      <td>0.00545</td>\n",
       "      <td>0.981982</td>\n",
       "      <td>88.070133</td>\n",
       "      <td>0.005388</td>\n",
       "      <td>54.881175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(question)</td>\n",
       "      <td>(compose)</td>\n",
       "      <td>0.02060</td>\n",
       "      <td>0.00555</td>\n",
       "      <td>0.00530</td>\n",
       "      <td>0.257282</td>\n",
       "      <td>46.357037</td>\n",
       "      <td>0.005186</td>\n",
       "      <td>1.338933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173585</th>\n",
       "      <td>(subreddit)</td>\n",
       "      <td>(perform, please, message, concern, automatica...</td>\n",
       "      <td>0.00660</td>\n",
       "      <td>0.00510</td>\n",
       "      <td>0.00510</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>151.515152</td>\n",
       "      <td>0.005066</td>\n",
       "      <td>4.377560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173586</th>\n",
       "      <td>(contact)</td>\n",
       "      <td>(perform, please, message, concern, automatica...</td>\n",
       "      <td>0.01245</td>\n",
       "      <td>0.00510</td>\n",
       "      <td>0.00510</td>\n",
       "      <td>0.409639</td>\n",
       "      <td>80.321285</td>\n",
       "      <td>0.005037</td>\n",
       "      <td>1.685239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173587</th>\n",
       "      <td>(moderator)</td>\n",
       "      <td>(perform, please, message, concern, automatica...</td>\n",
       "      <td>0.00550</td>\n",
       "      <td>0.00510</td>\n",
       "      <td>0.00510</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>181.818182</td>\n",
       "      <td>0.005072</td>\n",
       "      <td>13.679875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173588</th>\n",
       "      <td>(compose)</td>\n",
       "      <td>(perform, please, message, concern, automatica...</td>\n",
       "      <td>0.00555</td>\n",
       "      <td>0.00510</td>\n",
       "      <td>0.00510</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>180.180180</td>\n",
       "      <td>0.005072</td>\n",
       "      <td>12.270433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173589</th>\n",
       "      <td>(action)</td>\n",
       "      <td>(perform, please, message, concern, automatica...</td>\n",
       "      <td>0.01035</td>\n",
       "      <td>0.00510</td>\n",
       "      <td>0.00510</td>\n",
       "      <td>0.492754</td>\n",
       "      <td>96.618357</td>\n",
       "      <td>0.005047</td>\n",
       "      <td>1.961374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>173590 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        antecedents                                        consequents  \\\n",
       "0         (perform)                                          (compose)   \n",
       "1         (compose)                                          (perform)   \n",
       "2         (message)                                          (compose)   \n",
       "3         (compose)                                          (message)   \n",
       "4        (question)                                          (compose)   \n",
       "...             ...                                                ...   \n",
       "173585  (subreddit)  (perform, please, message, concern, automatica...   \n",
       "173586    (contact)  (perform, please, message, concern, automatica...   \n",
       "173587  (moderator)  (perform, please, message, concern, automatica...   \n",
       "173588    (compose)  (perform, please, message, concern, automatica...   \n",
       "173589     (action)  (perform, please, message, concern, automatica...   \n",
       "\n",
       "        antecedent support  consequent support  support  confidence  \\\n",
       "0                  0.00650             0.00555  0.00510    0.784615   \n",
       "1                  0.00555             0.00650  0.00510    0.918919   \n",
       "2                  0.01115             0.00555  0.00545    0.488789   \n",
       "3                  0.00555             0.01115  0.00545    0.981982   \n",
       "4                  0.02060             0.00555  0.00530    0.257282   \n",
       "...                    ...                 ...      ...         ...   \n",
       "173585             0.00660             0.00510  0.00510    0.772727   \n",
       "173586             0.01245             0.00510  0.00510    0.409639   \n",
       "173587             0.00550             0.00510  0.00510    0.927273   \n",
       "173588             0.00555             0.00510  0.00510    0.918919   \n",
       "173589             0.01035             0.00510  0.00510    0.492754   \n",
       "\n",
       "              lift  leverage  conviction  \n",
       "0       141.372141  0.005064    4.617089  \n",
       "1       141.372141  0.005064   12.253167  \n",
       "2        88.070133  0.005388    1.945284  \n",
       "3        88.070133  0.005388   54.881175  \n",
       "4        46.357037  0.005186    1.338933  \n",
       "...            ...       ...         ...  \n",
       "173585  151.515152  0.005066    4.377560  \n",
       "173586   80.321285  0.005037    1.685239  \n",
       "173587  181.818182  0.005072   13.679875  \n",
       "173588  180.180180  0.005072   12.270433  \n",
       "173589   96.618357  0.005047    1.961374  \n",
       "\n",
       "[173590 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rules = association_rules(freq_items, metric=\"confidence\", min_threshold=0.00001)\n",
    "display(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a search term: compose\n",
      "The top trends for the search term 'compose' are: \n",
      "perform\n",
      "message\n",
      "question\n",
      "concern\n",
      "moderator\n",
      "contact\n",
      "automatically\n",
      "action\n",
      "subreddit\n",
      "please\n"
     ]
    }
   ],
   "source": [
    "\n",
    "search_term = input(\"Enter a search term: \")\n",
    "\n",
    "while search_term not in keywords:\n",
    "    print(\"Could not find any related trends for the search term '\"+search_term +\"'\") \n",
    "    search_term = input(\"Enter a search term: \")\n",
    "\n",
    "print(\"The top trends for the search term '\"+search_term +\"' are: \")\n",
    "for term in keywords[search_term]:\n",
    "    print(term[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
